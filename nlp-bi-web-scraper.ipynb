{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/joshuaokolo/nlp-bi-web-scraper?scriptVersionId=104037536\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"from urllib.request import Request, urlopen\nfrom bs4 import BeautifulSoup as soup\nimport pandas as pd\n\n# Let's pick a company ticker symbol (AMZN for Amazon)\ncompany_ticker = 'AMZN'\n# Add the ticker symbol to the \"finviz\" search box url\nurl = (\"http://finviz.com/quote.ashx?t=\" + company_ticker.lower())\n# Most websites block requests that are without a User-Agent header (these simulate a typical browser)\n\n# Send a Request to the url and return an html file\nreq = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n\n# open and read the request\nwebpage = urlopen(req).read()\n\n# make a soup using BeautifulSoup from webpage\nhtml = soup(webpage, \"html.parser\")\n\n# Extract the 'class' = 'fullview-news-outer' from our html code, and create a dataframe from it\nnews = pd.read_html(str(html), attrs={'class': 'fullview-news-outer'})[0]\n\n# extract the links for each news by finding all the \"a\" tags and 'class' = 'tab-link-news'\nlinks = []\nfor a in html.find_all('a', class_=\"tab-link-news\"):\nlinks.append(a['href'])\n\n# Clean up our news dataframe\nnews.columns = ['Date', 'News_Headline']\nnews['Article_Link'] = links\nnews.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# extract time as a new column\nnews['time'] = news['Date'].apply(lambda x: ''.join(re.findall(r'[a-zA-Z]{1,9}-\\d{1,2}-\\d{1,2}\\s(.+)', x)))\n\n# fill empty cells by the times mentioned in the \"Date\" column\nnews.loc[news['time'] == '', 'time'] = news['Date']\n\nnews","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nnews['date'] = news['Date'].apply(lambda x: ''.join(re.findall(r'([a-zA-Z]{1,9}-\\d{1,2}-\\d{1,2})\\s.+', x)))\n\n# change empty cells to NaN type in the new \"date\" column\nnews.loc[news['date'] == '', 'date'] = np.nan\n\n# fillna() by forward filling\nnews.fillna(method = 'ffill', inplace = True)\n\nnews","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine \"date\" & \"time\" columns and convert to datetime type\nnews['datetime'] = pd.to_datetime(news['date'] + ' ' + news['time'])\n\n# clean out dataframe\nnews.drop(['Date', 'time', 'date'], axis = 1, inplace = True)\nnews.sort_values('datetime', inplace = True)\nnews.reset_index(drop=True, inplace =True)\nnews.columns = ['news_headline', 'url', 'datetime']\n\nNews","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from newsapi.newsapi_client import NewsApiClient\n\ncompany_ticker = 'AMZN'\nsearch_date = '2022-04-01'\n\nnewsapi = NewsApiClient(api_key='3a2d0a55066041dc81e3acfbd665fc6e')\n# extract \"articles\", which will be a dictionary\narticles = newsapi.get_everything(q=company_ticker,\n                              from_param=search_date,\n                              language=\"en\",\n                              sort_by=\"publishedAt\",\n                              page_size=100)\n\n# we want to get the \"articles\" key from our \"articles\" dictionary\ndf_newsapi = pd.DataFrame(articles['articles'])\ndf_newsapi.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do some cleaning of the df_newsapi\ndf_newsapi.drop(['author', 'urlToImage'], axis=1, inplace=True)\ndf_newsapi.rename({'publishedAt': 'datetime'}, axis=1, inplace = True)\ndf_newsapi.rename({'title': 'news_headline'}, axis=1, inplace = True)\ndf_newsapi['source'] =  df_newsapi['source'].map(lambda x: x['name'])\ndf_newsapi.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from GoogleNews import GoogleNews\nfrom newspaper import Config\nimport re\n\ncompany_ticker = 'AMZN'\nsearch_date = '2022-04-02'\n\n# GoogleNews sometime returns an empty dataframe, so we add a try and except Block for handling those exceptions\ntry:\nuser_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0'\nconfig = Config()\nconfig.browser_user_agent = user_agent\nconfig.request_timeout = 10\ndf_google = pd.DataFrame()\n\n# change the format of date string from YYYY-MM-DD to MM/DD/YYYY so that is works with GoogleNews\nstart_date = re.sub(r'(\\d{4})-(\\d{1,2})-(\\d{1,2})', '\\\\2/\\\\3/\\\\1', search_date)\n\n# Extract News with Google News ---> gives only 10 results per request\ngooglenews = GoogleNews(start=start_date)\ngooglenews.search(company_ticker)\n\n# store the results of the first result page\nresult1 = googlenews.result()\ndf_google1 = pd.DataFrame(result1)\n\n# store the results of the 2nd result page\ngooglenews.clear()\ngooglenews.getpage(2)\nresult2 = googlenews.result()\ndf_google2 = pd.DataFrame(result2)\n\ndf_google = pd.concat([df_google1, df_google2])\n\n# do some cleaning of the df_google DF\nif df_google.shape[0] != 0:\n    df_google.drop(['img', 'date'], axis=1, inplace=True)\n    df_google.columns = ['news_headline', 'source', 'datetime', 'description', 'url']\ndisplay(df_google.head())\nexcept:\npass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from urllib.request import Request, urlopen\nfrom bs4 import BeautifulSoup as soup\nimport pandas as pd\nimport re\nimport numpy as np\nfrom newsapi.newsapi_client import NewsApiClient\n\ndef get_news(company_ticker, search_date):\n    ## newsapi\n    newsapi = NewsApiClient(api_key='3a2d0a55066041dc81e3acfbd665fc6e')\n    articles = newsapi.get_everything(q=company_ticker,      \n                                   from_param=search_date,\n                                   language=\"en\",\n                                   sort_by=\"publishedAt\",\n                                   page_size=100)\n    df_newsapi = pd.DataFrame(articles['articles'])\n    # do some cleaning of the DF\n    df_newsapi.drop(['author', 'urlToImage'], axis=1, inplace=True)\n    df_newsapi.rename({'publishedAt': 'datetime'}, axis=1, inplace = True)\n    df_newsapi.rename({'title': 'news_headline'}, axis=1, inplace = True)\n    df_newsapi['source'] =  df_newsapi['source'].map(lambda x: x['name'])  \n    \n    ## finviz\n    url = (\"http://finviz.com/quote.ashx?t=\" + company_ticker.lower())\n    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n    webpage = urlopen(req).read()\n    html = soup(webpage, \"html.parser\")\n    news = pd.read_html(str(html), attrs={'class': 'fullview-news-outer'})[0]\n\n    links = []\n    for a in html.find_all('a', class_=\"tab-link-news\"):\n        links.append(a['href'])\n    # Clean up news dataframe\n    news.columns = ['Date', 'News_Headline']\n    news['Article_Link'] = links\n\n     # >>> clean \"Date\" column and create a new \"datetime\" column\n     # extract time\n    news['time'] = news['Date'].apply(lambda x: ''.join(re.findall(r'[a-zA-Z]{1,9}-\\d{1,2}-\\d{1,2}\\s(.+)', x)))\n    news.loc[news['time'] == '', 'time'] = news['Date']\n    #extract date\n    news['date'] = news['Date'].apply(lambda x: ''.join(re.findall(r'([a-zA-Z]{1,9}-\\d{1,2}-\\d{1,2})\\s.+', x)))\n    news.loc[news['date'] == '', 'date'] = np.nan\n    news.fillna(method = 'ffill', inplace = True)\n    # convert to datetime type\n    news['datetime'] = pd.to_datetime(news['date'] + ' ' + news['time'])\n    news.drop(['Date', 'time', 'date'], axis = 1, inplace = True)\n    news.sort_values('datetime', inplace = True)\n    news.reset_index(drop=True, inplace =True)\n    news.columns = ['news_headline', 'url', 'datetime']\n    df_finviz = news.copy()## GoogleNews# GoogleNews sometime returns an empty dataframe, so we add a try and except Block for handling those exceptions\ntry:\n    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0'\n    config = Config()\n    config.browser_user_agent = user_agent\n    config.request_timeout = 10\n    df_google = pd.DataFrame()\n\n    # change the format of date string from YYYY-MM-DD to MM/DD/YYYY so that is works with GoogleNews\n    start_date = re.sub(r'(\\d{4})-(\\d{1,2})-(\\d{1,2})', '\\\\2/\\\\3/\\\\1', search_date)\n\n    # Extract News with Google News ---> gives only 10 results per request\n    googlenews = GoogleNews(start=start_date)\n    googlenews.search(company_ticker)\n    \n    # store the results of the first result page\n    result1 = googlenews.result()\n    df_google1 = pd.DataFrame(result1)\n    \n    # store the results of the 2nd result page\n    googlenews.clear()\n    googlenews.getpage(2)\n    result2 = googlenews.result()\n    df_google2 = pd.DataFrame(result2)\n    \n    df_google = pd.concat([df_google1, df_google2])\n    # do some cleaning of the df_google DF\n    if df_google.shape[0] != 0:\n        df_google.drop(['img', 'date'], axis=1, inplace=True)\n        df_google.columns = ['news_headline', 'source', 'datetime', 'description', 'url']\nexcept:\n    pass\n    ## Add the 3 DFs together\n     df_news = pd.concat([df_newsapi, df_finviz, df_google], ignore_index=True)\n    df_news['datetime'] = pd.to_datetime(df_news['datetime'], format = '%Y-%m-%d %H:%M:%S')\n    df_news.set_index('datetime', inplace = True)\n    # only returning the rows that match our search_date\n    df_news = df_news[df_news.index.to_period('D') == search_date]\n    df_news.sort_index(inplace = True)\n    # Get clean source column from urls using regex\n    df_news['source'] = df_news['url'].map(lambda x: ''.join(re.findall(r\"https?://(?:www.)?([A-Za-z_0-9.-]+).*\", x)))\n    \n    return df_news","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_news = get_news('AMZN', '2022-04-01')\n\ndf_news.shape\n\n>>> (68, 5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n# download these 3 when run for the first time\nnltk.download('vader_lexicon')\nnltk.download('movie_reviews')\nnltk.download('punkt')\n\ndef nltk_vader_score(text):\nsentiment_analyzer = SIA()\n# we take \"compound score\" (from -1 to 1): The normalized compound score which calculates the sum of all lexicon ratings\nsent_score = sentiment_analyzer.polarity_scores(text)['compound']\nreturn sent_score\n\ndf_news['sentiment_score_vader'] = df_news['news_headline'].map(nltk_vader_score)\ndf_news.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nfig = px.histogram(\ndf_news, x='sentiment_score_vader',\ncolor='source').update_xaxes(categoryorder=\"total descending\")\n\nfig.update_layout(xaxis_title='Sentiment Score (Compound from -1 to 1)',\n              yaxis_title='Count',\n              font=dict(size=16),\n              bargap=0.025,\n              width=790,\n              height=520,\n              legend=dict(orientation=\"h\",\n                          yanchor=\"top\",\n                          y=1.23,\n                          xanchor=\"center\",\n                          x=0.48))\nfig.show('notebook')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentiment_type(text):\nanalyzer = SIA().polarity_scores(text)\nneg = analyzer['neg']\nneu = analyzer['neu']\npos = analyzer['pos']\ncomp = analyzer['compound']\n   \nif neg > pos:\n    return 'negative'\nelif pos > neg:\n    return 'positive'\nelif pos == neg:\n    return 'neutral'df_news['sentiment_type'] = df_news['news_headline'].map(sentiment_type)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.pie(df_news,\n        values=df_news['sentiment_type'].value_counts(normalize=True) * 100,\n        names=df_news['sentiment_type'].unique(),\n        color=df_news['sentiment_type'].unique(),\n        hole=0.35,\n        color_discrete_map={\n            'neutral': 'silver',\n            'positive': 'mediumspringgreen',\n            'negative': 'orangered'\n        })fig.update_traces(textposition='inside', textinfo='percent+label', textfont_size=22, hoverinfo='label+value',\n              texttemplate = \"%{label}<br>%{value:.0f}%\")fig.update_layout(font=dict(size=16),\n              width=810,\n              height=520)\nfig.show('notebook')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\ndef word_cloud(text):\nstopwords = set(STOPWORDS)\nallWords = ' '.join([nws for nws in text])\nwordCloud = WordCloud(\n    background_color='white',  # black\n    width=1600,\n    height=800,\n    stopwords=stopwords,\n    min_font_size=20,\n    max_font_size=150).generate(allWords)\nfig, ax = plt.subplots(figsize=(20, 10),\n                      facecolor='w')  # facecolor='k' for black frame\nplt.imshow(wordCloud, interpolation='bilinear')\nax.axis(\"off\")\nfig.tight_layout(pad=0)\nplt.show()\n\nprint('Wordcloud for ' + company_ticker)\nword_cloud(df_news['news_headline_tokens'].values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"soup = BeautifulSoup(html_text, 'lxml')\ntag = soup.body","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_article_text(Article_Link):\nimport requests\nfrom bs4 import BeautifulSoup\n\n# using request package to make a GET request for the website, which means we're getting data from it.\nheader = {\n    \"User-Agent\":\n    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n    \"X-Requested-With\": \"XMLHttpRequest\"\n}\n\nhtml = requests.get(Article_Link, headers=header).content\nsoup = BeautifulSoup(html)\n\n# Get the whole body tag\ntag = soup.body\n\n# Join each string recursively\ntext = []\nfor string in tag.strings:\n    # ignore if fewer than 15 words\n    if len(string.split()) > 15:\n        text.append(string)\nreturn ' '.join(text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_news['news_text'] = df_news['url'].map(get_article_text)\n\n# cleaning news_text by transforming anything that is NOT space, letters, or numbers to ''\ndf_news['news_text'] = df_news['news_text'].apply(lambda x: re.sub('[^ a-zA-Z0-9]', '', x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def keyword_extractor(text):\nfrom flashtext import KeywordProcessor\nkwp = KeywordProcessor()\n\nkeyword_dict = {\n    'new product': ['new product', 'new products'],\n    'M&A': ['merger', 'acquisition'],\n    'stock split/buyback': ['buyback', 'split'],\n    'workforce change': ['hire', 'hiring', 'firing', 'lay off', 'laid off']\n}\n\nkwp.add_keywords_from_dict(keyword_dict)\n   \n# we use set to get rid of repeating keywords, and ', '.join() to get string instead of SET data type:\nreturn ', '.join(set(kwp.extract_keywords(text)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_news['event_keywords'] = df_news['news_text'].map(keyword_extractor)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(\n    df_news[df_news['event_keywords'] != ''],\n    x='event_keywords',\n    color='sentiment_type',\n    color_discrete_map={\n                        'neutral': 'silver',\n                        'positive': 'mediumspringgreen',\n                        'negative': 'orangered'\n                    }).update_xaxes(categoryorder=\"total descending\")\n\nfig.update_layout(yaxis_title='Count',\n              xaxis_title='',\n              width=810,\n              height=620,\n              font=dict(size=16),\n              legend=dict(orientation=\"h\",\n                          yanchor=\"top\",\n                          y=1.16,\n                          xanchor=\"center\",\n                          x=0.5))\nfig.update_xaxes(tickangle=-45)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**References**:\n\nhttps://towardsdatascience.com/the-best-python-sentiment-analysis-package-1-huge-common-mistake-d6da9ad6cdeb\n\nhttps://pythoninvest.com/long-read/sentiment-analysis-of-financial-news\n\nhttps://www.kaggle.com/mmmarchetti/sentiment-analysis-on-financial-news\n\nhttps://medium.datadriveninvestor.com/scraping-live-stock-fundamental-ratios-news-and-more-with-python-a716329e0493\n\nhttps://omdena.com/blog/business-intelligence-tool-for-financial/\n\nhttps://pypi.org/project/finpie/\n\nhttps://tradewithpython.com/news-sentiment-analysis-using-pytho","metadata":{}}]}